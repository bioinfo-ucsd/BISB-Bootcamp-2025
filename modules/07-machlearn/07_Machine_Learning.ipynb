{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59f52be",
   "metadata": {},
   "source": [
    "# Machine Learning Basics\n",
    "\n",
    "Adapted from the Python Data Science Handbook: https://jakevdp.github.io/PythonDataScienceHandbook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba430ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6cdb9",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Unsupervised Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed551f61",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade0d4f",
   "metadata": {},
   "source": [
    "A clustering algorithm attempts to find distinct groups of data without reference to any labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25dba2",
   "metadata": {},
   "source": [
    "#### [K-means Clustering](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "\n",
    "The k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. The typical approach to k-means involves an intuitive iterative approach known as expectation–maximization (E-M), an iterative process by which a center is guessed, points are assigned to the nearest cluster, and then the center is set to the points mean. \n",
    "\n",
    "It accomplishes this using a simple conception of what the optimal clustering looks like:\n",
    "\n",
    "- The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "- Each point is closer to its own cluster center than to other cluster centers.\n",
    "\n",
    "Given simple, well-separated data, k-means finds suitable clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=400, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "X = X[:, ::-1] # flip axes for better plotting\n",
    "\n",
    "# Plot the Raw data\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.show()\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 4\n",
    "\n",
    "# Plot the data with K Means Labels\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(NUMBER_OF_CLUSTERS, random_state=0)\n",
    "labels = kmeans.fit(X).predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d84e9-79c4-4883-b104-2fe6e1a0df2d",
   "metadata": {},
   "source": [
    "#### How many clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19f3be-b690-49ec-9973-4678725bb0d4",
   "metadata": {},
   "source": [
    "The inertia measures how close each node is to the centroid of its cluster. We can use inertia to identify what number of clusters optimizes this metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c756fe2-0e9d-453b-bc1b-c2c4791028a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The sum of squared distances is {} with {} clusters\".format(kmeans.inertia_,NUMBER_OF_CLUSTERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1f96c-6858-4670-aaf3-04fdc2713cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(2,10)\n",
    "Sum_of_squared_distances = []\n",
    "Silhouette_scores = []\n",
    "for num_clusters in K:\n",
    "    kmeans = KMeans(num_clusters, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "    Silhouette_scores.append(silhouette_score(X,kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010d022-6008-490d-9725-80f78bfd54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K,Sum_of_squared_distances, 'o-')\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.xlabel(\"Number of clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903970d5-6e73-45c6-8364-7102eed085d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K,Silhouette_scores, 'o-')\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.xlabel(\"Number of clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3726b6",
   "metadata": {},
   "source": [
    "Two disadvantages of k-means\n",
    "* Its lack of flexibility in cluster shape\n",
    "* Lack of probabilistic cluster assignment, mean that for many datasets (especially low-dimensional datasets) it may not perform as well as you might hope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f3e35",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Models ([GMM](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html))\n",
    "\n",
    "To overcome the weakness of the k-means model, we can use the Gaussian mixture model (GMM) which attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. In the simplest case, GMMs can be used for finding clusters in the same manner as k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "gmm = mixture.GaussianMixture(n_components=4).fit(X)\n",
    "labels = gmm.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f267aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probability that each point belongs in the cluster\n",
    "probs = gmm.predict_proba(X)\n",
    "print(probs[:5].round(3)) # Print Subset of probabilities\n",
    "\n",
    "# Use probabilities to visualize certainty\n",
    "size = 50 * probs.max(1) ** 2  # square emphasizes differences\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472be465",
   "metadata": {},
   "source": [
    "But because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the predict_proba method. This returns a matrix of size [n_samples, n_clusters] which measures the probability that any point belongs to the given cluster:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad29de4",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303cbdd",
   "metadata": {},
   "source": [
    "#### Principle Component Analysis ([PCA](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528b19b",
   "metadata": {},
   "source": [
    "The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data. Often dimensionality reduction is used as an aid to visualizing data since it is much easier to plot data in two dimensions than in four dimensions or higher.\n",
    "\n",
    "PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more. A unsupervised learning problem attempts to learn about the relationship between the x and y values and this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ff39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c81a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)\n",
    "sum_eigen = sum(pca.explained_variance_)\n",
    "\n",
    "print(\"Eigenvalues are: {}\".format(pca.explained_variance_))\n",
    "print(\"PC1 has elements: {} and explains {:.2f}% of the variance\".format(pca.components_[0],100*pca.explained_variance_[0]/sum_eigen))\n",
    "print(\"PC2 has elements: {} and explains {:.2f}% of the variance\".format(pca.components_[1],100*pca.explained_variance_[1]/sum_eigen))\n",
    "print(\"Norms of PC1 and PC2 are {} and {}\".format(sum(i**2 for i in pca.components_[0]),\n",
    "                                                       sum(i**2 for i in pca.components_[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d5b52",
   "metadata": {},
   "source": [
    "To understand these numbers, we can visualize them as vectors over the input data\n",
    "Using the \"components\" to define the direction of the vector, \n",
    "and the \"explained variance\" to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# Instantiate Figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# plot data with PCs and explained variance vectors\n",
    "ax[0].scatter(X[:, 0], X[:, 1], alpha=0.2) # raw data\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length) # 3*Standard Dev = 99% of distribution\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
    "ax[0].axis('equal');\n",
    "ax[0].set(xlabel='x', ylabel='y', title='input')\n",
    "\n",
    "# plot the projection of each data point onto the principal axes \n",
    "# a.k.a the principal components of the data\n",
    "X_pca = pca.transform(X)\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2) # transformed data\n",
    "draw_vector([0, 0], [0, 3], ax=ax[1])\n",
    "draw_vector([0, 0], [3, 0], ax=ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlabel='component 1', ylabel='component 2',\n",
    "          title='principal components',\n",
    "          xlim=(-5, 5), ylim=(-3, 3.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924caf2",
   "metadata": {},
   "source": [
    "These vectors represent the principal axes of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis. \n",
    "\n",
    "The projection of each data point onto the principal axes are the \"principal components\" of the data.\n",
    "\n",
    "From here, we can use PCA as a method of dimensionality reduction which involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance. The transformed data is reduced to a single dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97112066",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e846f0",
   "metadata": {},
   "source": [
    "To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b416dd",
   "metadata": {},
   "source": [
    "The blue points are the original data, while the orange points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4621f",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Supervised Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c19c4",
   "metadata": {},
   "source": [
    "### Linear Regression ([LR](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html))\n",
    "\n",
    "Linear regression models are a good starting point for regression tasks. Such models are popular because they can be fit very quickly, and are very interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data scattered about a line with a slope of 2 and an intercept of -5:\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432359c",
   "metadata": {},
   "source": [
    "We can use Scikit-Learn's LinearRegression estimator to fit this data and construct the best-fit line.\n",
    "\n",
    "The slope and intercept of the data are contained in the model's fit parameters, which in Scikit-Learn are always marked by a trailing underscore. The relevant parameters are coef_ and intercept_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c80f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e62f4",
   "metadata": {},
   "source": [
    "LR can also handle multidimensional linear models where there are multiple x values. Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29091bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3) # 3 ind. variables\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e54df5",
   "metadata": {},
   "source": [
    "### Random Forest ([RF](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html))\n",
    "\n",
    "Random forests are an example of an ensemble learner built on decision trees. In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features.\n",
    "\n",
    "We will use the hand-written digits dataset as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98740df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize dataset\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ebfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aaec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can classify the digits using RF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Split the data into train, and test\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
    "                                                random_state=0)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)\n",
    "\n",
    "# Print performance metrics\n",
    "print(metrics.classification_report(ypred, ytest))\n",
    "\n",
    "# And performance confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c7499",
   "metadata": {},
   "source": [
    "# Single-Cell Sequencing Analysis\n",
    "\n",
    "Single-cell dataset and specific preprocessing from: https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7471447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the count matrix into an AnnData object, \n",
    "# which holds many slots for annotations and different representations of the data. \n",
    "# It also comes with its own HDF5-based file format: .h5ad\n",
    "adata_orig = sc.datasets.pbmc3k()\n",
    "adata_orig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata_orig.copy()\n",
    "\n",
    "# Perform basic filtering\n",
    "adata.var_names_make_unique()\n",
    "# filters out 19024 genes that are detected in less than 3 cells\n",
    "sc.pp.filter_cells(adata, min_genes=200) \n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "\n",
    "# QC\n",
    "adata.var['mt'] = adata.var_names.str.startswith('MT-')  # annotate the group of mitochondrial genes as 'mt'\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True) # Computes many qc metrics\n",
    "adata = adata[adata.obs.n_genes_by_counts < 2500, :] # Remove cells that have too many total counts\n",
    "pre_mt_filter = adata.obs.copy() # save for visualization later\n",
    "adata = adata[adata.obs.pct_counts_mt < 5, :] # Remove cells that have too many mitochondrial genes expressed \n",
    "\n",
    "expr_raw = adata.to_df() # store raw counts for DE testing\n",
    "adata_unsupervised = adata.copy()\n",
    "\n",
    "# processing\n",
    "sc.pp.normalize_total(adata, target_sum=1e6) # normalize to counts-per-million (CPM)\n",
    "sc.pp.log1p(adata) # logarithmize\n",
    "\n",
    "# filter for highly variable genes (HVGs)\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "adata = adata[:, adata.var.highly_variable]\n",
    "\n",
    "# transfer annotations of processed data\n",
    "processed_data = sc.datasets.pbmc3k_processed()\n",
    "adata.obs['louvain'] = processed_data.obs['louvain']\n",
    "\n",
    "adata_check = adata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399546e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the filtered expression data in an expression dataframe (matrix)\n",
    "expr = adata.to_df()\n",
    "# Store cell metatdata \n",
    "cell_md = adata.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0f748",
   "metadata": {},
   "source": [
    "The expression matrix is a sparse cell by gene matrix containing log-normalized UMI counts of single-cell RNAseq measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216581f2",
   "metadata": {},
   "source": [
    "The cell metadata contains quantitative and categorical information associated with each individual measured cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_md.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118995e",
   "metadata": {},
   "source": [
    "# Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a50c4d",
   "metadata": {},
   "source": [
    "We will scale each feature so that expression is comparable across them. In this case, we will simply use z-scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis = 0 specifies that z-scoring is done for each gene across cells rather than vice-versa\n",
    "expr_scaled = stats.zscore(expr, axis = 0) \n",
    "\n",
    "# Save as DF with gene names as column names\n",
    "expr_scaled_df = pd.DataFrame(expr_scaled, columns = expr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1b0696",
   "metadata": {},
   "source": [
    "Differences in gene distributions across all cells are less pronounced after scaling, ensuring that not just highly expressed genes are biasing results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = ['QRICH1', 'ESCO2'] # genes with max and min means across all cells\n",
    "\n",
    "fig, ax = plt.subplots(ncols = 2, figsize = (10,5), sharey = True)\n",
    "sns.kdeplot(data = expr.loc[:, genes], ax = ax[0])\n",
    "sns.kdeplot(data = expr_scaled_df.loc[:, genes], ax = ax[1]) #\n",
    "\n",
    "ax[0].set_title('Unscaled Data')\n",
    "ax[1].set_title('Scaled Data')\n",
    "for i in range(2):\n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_xlabel('Gene Expression')\n",
    "    ax[i].get_legend().set_title('Gene ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6d30d",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc56b8d",
   "metadata": {},
   "source": [
    "## Continuous Variables for QC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4e37a",
   "metadata": {},
   "source": [
    "The fraction of total genes that represent mitochondrial genes in a given cell is a QC metric. A high mitochondrial percentage indicates a cell whose cytoplasmic mRNA has leaked out through a broken membrane, and thus, only mRNA located in the mitochondria is still conserved.\n",
    "\n",
    "A high total # of counts in a cell is also indicative of a low-quality cell, because it indicates that the measured droplet possibly contained a doublet.\n",
    "\n",
    "These QC metrics were calculated by scanpy in the single-cell preprocessing steps and can be observed in the cell metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_md[['pct_counts_mt', 'total_counts']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6b8e5",
   "metadata": {},
   "source": [
    "A common preprocessing step is to simply filter out low-quality cells using a heuristic threshold, e.g. cells with >5% of counts being mitochondrial as done in line 12 of the single-cell preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(y = pre_mt_filter['pct_counts_mt'])\n",
    "ax.axhline(5, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f89ae",
   "metadata": {},
   "source": [
    "An alternative method is to use regression to correct for technical variance associated with QC metrics. This may be done in combination with the heuristic threshold applied above, as exemplified in the scanpy tutorial.\n",
    "\n",
    "Some notes:\n",
    " - Due to the additional complexities of modeling sparse data, we will consider simpler scenarios in which we disregard 0 counts; this step is only included for the case of this simple example and would not be done in actual single-cell analysis.bullte\n",
    " - We will not use the scaled data, since the correction method also works as a scaling method\n",
    "\n",
    "Let's explore this idea with one gene. First, let's calculate which gene has the highest correlation with the mitochondrial percentage across cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079eb9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = 'pct_counts_mt'\n",
    "def regressor_pearson(counts, regressor = regressor, cell_thresh = 10, cell_md = cell_md):\n",
    "    counts_ = counts[counts != 0] # drop zero counts\n",
    "    \n",
    "    if counts_.shape[0] >= cell_thresh: # only consider those with at least cell_thresh non-zero cells\n",
    "        regressor_ = cell_md.loc[counts_.index, regressor] # only consider non-zero counts for technical covariate\n",
    "        pearson_r = stats.pearsonr(counts_, regressor_).statistic # get the pearsonr\n",
    "    else:\n",
    "        pearson_r = 0\n",
    "    return pearson_r\n",
    "\n",
    "pearson_correlation = expr.apply(lambda counts: regressor_pearson(counts)) # calculate for each gene\n",
    "\n",
    "# see which gene has highest correlation\n",
    "print('The gene with the highest correlation is {} with a Pearson r of {:.3f}'.format(pearson_correlation.idxmax(), \n",
    "                                                                                     pearson_correlation.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ea3a9",
   "metadata": {},
   "source": [
    "Next, let's build the linear regression model for this gene: gene ~ B_0 + B_1*pct_counts_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb85537",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'DLEU7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5970d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter expression data for non-zero counts for the selected gene \n",
    "counts_ = expr[expr[gene] != 0][gene]\n",
    "# keep pct_counts_mt md for only the indices with non-zero counts\n",
    "cell_md_ = cell_md.loc[counts_.index, :][[regressor]]\n",
    "\n",
    "# do the regression\n",
    "lr = LinearRegression() #initialize the class\n",
    "lr.fit(y = counts_, X = cell_md_) # fit the regression model to the data\n",
    "\n",
    "print('LR formula: {} ~ {:.3f} + {:.3f}*MITO'.format(gene, lr.intercept_, lr.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497e234",
   "metadata": {},
   "source": [
    "We can visualize this as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "viz_df = pd.DataFrame(data = {gene: counts_, regressor: cell_md_[regressor]})\n",
    "display(viz_df)\n",
    "\n",
    "# plot the data\n",
    "sns.scatterplot(data = viz_df, y = gene, x = regressor, ax = ax) \n",
    "\n",
    "# plot the regression\n",
    "min_x, max_x = viz_df[regressor].min(), viz_df[regressor].max()\n",
    "x = np.linspace(min_x, max_x)\n",
    "ax.plot(x, lr.coef_[0]*x + lr.intercept_, color = 'red')\n",
    "\n",
    "# annotate the regression line\n",
    "ax.annotate('{} ~ {:.3f} + {:.3f}*MITO'.format(gene, lr.intercept_, lr.coef_[0]), \n",
    "           xy = (3, 7))\n",
    "ax.set_xlim(min_x - 0.2, max_x + 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda3e8e",
   "metadata": {},
   "source": [
    "To control for this, we can replace the count values with the residuals of the regression (i.e., the difference between scatter point and the red line on the graph above). This residual represents the actual biological variation in the data after correcting for the technical variation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da385377",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_df = cell_md_.copy()\n",
    "\n",
    "# get the model-estimated value for the gene count at each mitochondrial percentage value\n",
    "predicted_counts = lr.predict(cell_md_)\n",
    "\n",
    "residuals_df['Actual_Counts'] = counts_\n",
    "residuals_df['Predicted_Counts'] = predicted_counts\n",
    "\n",
    "# get the difference between the actual and model-estimated values\n",
    "residuals_df['Corrected_Counts'] = residuals_df.Actual_Counts - residuals_df.Predicted_Counts\n",
    "\n",
    "residuals_df.sort_values(by = regressor, inplace = True)\n",
    "residuals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643666c",
   "metadata": {},
   "source": [
    "We can see that the correlation between the new corrected counts and the mitochondrial percentage is now gone:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = stats.pearsonr(residuals_df.Corrected_Counts, residuals_df[regressor]).statistic\n",
    "print('The correlation after correction is {:.2e}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af3c3b",
   "metadata": {},
   "source": [
    "We can see that the residual values no longer correlate with mitochondrial percentage, and also that the values are scaled. Note also that this essentially rotates that data about the ordinary least squares (OLS) line of best fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33432c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot the data\n",
    "sns.scatterplot(data = residuals_df, y = 'Corrected_Counts', x = regressor, ax = ax) \n",
    "ax.axhline(0, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bef7ab",
   "metadata": {},
   "source": [
    "Since we are considering two QC metrics (mitochondrial % & total counts), our actual model will actually be a multi-linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = ['pct_counts_mt', 'total_counts']\n",
    "cell_md_ = cell_md.loc[counts_.index, regressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the regression\n",
    "lr = LinearRegression() #initialize the class\n",
    "lr.fit(y = counts_, X = cell_md_) # fit the regression model to the data\n",
    "\n",
    "print('LR formula: {} ~ {:.3f} + {:.3f}*MITO + {:.3f}*TOTAL_COUNTS'.format(gene, lr.intercept_, lr.coef_[0], \n",
    "                                                                           lr.coef_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab2957",
   "metadata": {},
   "source": [
    "In this particular example, it seems that the total counts has a small effect size (i.e., does not contribute much to the technical variation in the data). We can proceed with repeating the correction process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model-estimated value for the gene count at each mitochondrial percentage value\n",
    "predicted_vals = lr.predict(cell_md_)\n",
    "# get the difference between the actual and model-estimated values\n",
    "corrected_counts = counts_ - predicted_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce2863",
   "metadata": {},
   "source": [
    "## Categorical Variables for Differential Expression Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8995a7ee",
   "metadata": {},
   "source": [
    "Differential expression tests for relative differences in gene expression between two conditions. Regression analysis is often used for this purpose to account for the distribution of the data.    \n",
    "\n",
    "Here, let's try to identify markers of CD8+ T-cells, which involves testing for DE genes of cells annotated as CD8+ T-cells vs the rest of cells. Note that we have cell type annotations (the categorical variable of interest) from the processed data. Typically, we would first use an unsupervised learning approach to get cell clusters (see below), and then proceed to annotate the cell types from the cluster markers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e893b3",
   "metadata": {},
   "source": [
    "Some words on more complicated topics that we won't cover: \n",
    "\n",
    "1. Regression of categorical data with multiple categories is  analogous to ANOVAs. We do not show it here, but if you had 3 sample conditions (e.g., control, treatment A, treatment B), you could run a linear regression testing the different between each group mean and the control. If there is no control (treatments A-c), you can change the contasts to test for the different between the group and the grand mean\n",
    "2. We will use a simple linear regression, but most differential expresion tools will use a generalized linear model (GLM) that more accurately emulates the data distribution (typically negative binomial) and eliminates some assumptions of simple linear regression. \n",
    "3. Furthermore, these models will often use a multi-variate regression to introduce a covariate that can account for technical confounders (e.g., a categorical variable annotating the batch or a continuous latent variable capturing the technical variance)\n",
    " \n",
    "Because of points 2-3, DE testing is often done using the raw rather than processed counts, unfiltered for HVGs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845220dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_md.louvain.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e27790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the barcode of cells annotated as cd8\n",
    "cd8_barcodes = cell_md[cell_md.louvain == 'CD8 T cells'].index\n",
    "other_barcodes = cell_md[cell_md.louvain != 'CD8 T cells'].index\n",
    "\n",
    "# filter the raw data for annotated cell types, and order according to condition\n",
    "expr_raw = pd.concat([expr_raw.loc[cd8_barcodes, :], expr_raw.loc[other_barcodes, :]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d0ccf",
   "metadata": {},
   "source": [
    "Let's begin with one gene as a simple example. We will proceed with granzyme A, a cytolytic molecule commonly secreted by CD8+ T-cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'GZMA'\n",
    "# format the data\n",
    "reg_df = expr_raw[[gene]]\n",
    "reg_df['Cell_Type'] = (['CD8'] * cd8_barcodes.shape[0]) + (['other'] * other_barcodes.shape[0])\n",
    "\n",
    "reg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8fcbc",
   "metadata": {},
   "source": [
    "We can see that there are in fact expression differences for granzyme A in CD8 T cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b733aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(data = reg_df, y = gene, x = 'Cell_Type', ax = ax)\n",
    "ax.set_ylim(ax.get_ylim()[0], 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328810d",
   "metadata": {},
   "source": [
    "Now, let's create the contasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf8052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering the categorical data will ensure that we are testing for CD8 expression relative to the rest, rather\n",
    "# than vice-versa\n",
    "reg_df['Cell_Type'] = pd.Categorical(reg_df['Cell_Type'], \n",
    "                                     ordered=True, categories=['other', 'CD8'])\n",
    "\n",
    "# get the dummy variables\n",
    "# dropping the first category resolves issues of multi-collinearity\n",
    "# the first category is instead encoded by all 0s (the intercept)\n",
    "reg_df['Contrast'] = pd.get_dummies(reg_df.Cell_Type, drop_first = True).iloc[:, 0]\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb60b60",
   "metadata": {},
   "source": [
    "#### Aside\n",
    "\n",
    "imagine we had 3 conditions - treatment A, treatment B, and a control. Then, the contrasts would be more complicated and the regression would include more coefficients (gene ~ TreatA + TreatB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = reg_df.copy()\n",
    "\n",
    "random.seed(888)\n",
    "conditions = ['Control', 'TreatA', 'TreatB']\n",
    "treatment = random.choices(conditions, k = reg_df.shape[0])\n",
    "example_df['Treatment'] = pd.Categorical(treatment, ordered=True, categories=conditions)\n",
    "\n",
    "pd.concat([example_df[[gene, 'Treatment']], pd.get_dummies(example_df.Treatment, drop_first = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d762b",
   "metadata": {},
   "source": [
    "Now that we have our contrast, we can run the regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a018c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the regression\n",
    "lr = LinearRegression() #initialize the class\n",
    "lr.fit(y = reg_df[gene], X = reg_df[['Contrast']]) # fit the regression model to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903be596",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "1. The intercept is the expected mean of granzyme A expression in the non CD8Tcells (when Contrast = 0)\n",
    "2. The coefficient is the expected fold-change in granzyme A expression in CD8 T-cells relative to the rest of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The regression intecept is {:.2f}'.format(lr.intercept_))\n",
    "\n",
    "mean_gzm_other = reg_df[reg_df.Cell_Type == 'other'][gene].mean()\n",
    "print('The average expression of granzyme A in non CD8+ T-cells is is {:.2f}'.format(mean_gzm_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The regression coefficient is {:.2f}'.format(lr.coef_[0]))\n",
    "\n",
    "mean_gzm_cd8 = reg_df[reg_df.Cell_Type == 'CD8'][gene].mean()\n",
    "log_fold_change = np.log(mean_gzm_cd8/mean_gzm_other)\n",
    "\n",
    "print('The log-fold-change in average expression of granzyme A between CD8+ T-cells and the rest is {:.2f}'.format(log_fold_change))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09a951",
   "metadata": {},
   "source": [
    "Furthermore, regression coefficient's have a p-value associated with them (t-test or Wald's test), so we have both the effect size (coefficient value) and significance (p-value) of the differential expression for this gene. DE testing simply extends this to all genes (must add a multiple test correction for the p-values). Use the statsmodels package to get the p-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e80d4",
   "metadata": {},
   "source": [
    "Again, note that to correct for some technical covariate, we could simply add it to the regression model and follow the same interpretation for the Contrast coefficient we already incorporated here (gene ~ B_0 + B_1 x Contrast + B_2 x Technical_Covariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b49a82",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c1bd7",
   "metadata": {},
   "source": [
    "### Principle Component Analysis (PCA)\n",
    "We can reduce the dimensionality of the data by running principal component analysis (PCA), which reveals the main axes of variation and denoises the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29666208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are changing the preprocessing of the single-cell data just a little\n",
    "# to align with the unsupervised scanpy tutorial\n",
    "sc.pp.normalize_total(adata_unsupervised, target_sum=1e4)\n",
    "sc.pp.log1p(adata_unsupervised)\n",
    "sc.pp.highly_variable_genes(adata_unsupervised, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "adata_unsupervised = adata_unsupervised[:, adata_unsupervised.var.highly_variable]\n",
    "sc.pp.regress_out(adata_unsupervised, ['total_counts', 'pct_counts_mt'])\n",
    "sc.pp.scale(adata_unsupervised, max_value=10)\n",
    "\n",
    "# Saving the new unsupervised scaled ata\n",
    "expr_scaled_unsupervised = adata_unsupervised.to_df()\n",
    "expr_scaled_unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70767977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PCA from the sklearn.decomposition package\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100) # initialize the class to determine first 100 components\n",
    "pca_data = pca.fit_transform(expr_scaled_unsupervised)\n",
    "display(pca_data.shape)\n",
    "\n",
    "# Can get the values of explained variation for each PC\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "# Plot amount of variance explained by each of the components\n",
    "plt.scatter(range(0,100), pca.explained_variance_, alpha=0.2)\n",
    "plt.title('Amount Variance Explained')\n",
    "plt.xlabel('PCA')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f38f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first two principle components\n",
    "sns.scatterplot(x=pca_data[:,0], y=pca_data[:,1])\n",
    "plt.title('PCA of First Two Components')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9dc34c",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bcf63d",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85496978",
   "metadata": {},
   "source": [
    "We can incorporate the newly obtained PCA scores in the K-means algorithm. \n",
    "That's how we can perform classification based on principal components scores instead of the original features by using the scores obtained by the PCA for the K-means model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1968ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PCA from the sklearn.decomposition package\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cell_types = len(cell_md.louvain.unique()) # no. of cell types defined in processed dataset\n",
    "n_pcs = 10 # no. of pcs with high variance explained (elbow analysis above)\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters = n_cell_types, init = 'k-means++', random_state=42) # initialize the class to create 4 clusters\n",
    "x_clustered = kmeans_pca.fit_predict(pca_data[:,:n_pcs])\n",
    "display(x_clustered.shape)\n",
    "\n",
    "# Save the first two components PCA scores and the cluster number in a data frame\n",
    "pca_data_df = pd.DataFrame(pca_data)\n",
    "pca_data_df = pca_data_df.iloc[:,0:2]\n",
    "pca_data_df.columns = ['PC1', 'PC2']\n",
    "pca_data_df['cluster'] = x_clustered\n",
    "pca_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c676ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with the PCA with the cluster labels\n",
    "import seaborn as sns\n",
    "sns.color_palette(\"hls\", 8)\n",
    "sns.scatterplot(data=pca_data_df, x='PC1', y='PC2', hue='cluster', palette='tab10')\n",
    "plt.title('PCA of First Two Components with Cluster Labels')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a244321",
   "metadata": {},
   "source": [
    "# Classification: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4745a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8dd51",
   "metadata": {},
   "source": [
    "A challenging step in single-cell analysis is annotating cells with relevant cell types. One approach is to use a classifier to transfer lables from an annotated dataset to the dataset of interest, using the genes as features for the classifier. \n",
    "\n",
    "Here, we will train a random forest classifier on a subset of the data to annotate the remainder of the cells. Because all cells are already annotated, we can assess the performance. To make the problem even simpler, we will only consider a binary classification problem: T-cell vs not T-cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5479463",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_md = adata.obs\n",
    "\n",
    "# binarize the data\n",
    "binary_celltypes = {'CD4 T cells': 1, \n",
    "                    'B cells': 0, \n",
    "                    'CD14+ Monocytes': 0, \n",
    "                    'NK cells': 0, \n",
    "                    'CD8 T cells': 1, \n",
    "                    'FCGR3A+ Monocytes': 0, \n",
    "                    'Dendritic cells': 0, \n",
    "                    'Megakaryocytes': 0}\n",
    "cell_type = cell_md.louvain.map(binary_celltypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135cf006",
   "metadata": {},
   "source": [
    "Generate the training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data s.t. a fraction of samples are assigned to training and a fraction are assigned to testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(expr_scaled, cell_type, \n",
    "                                                    train_size = 0.8, test_size = 0.2, \n",
    "                                                   random_state = 888)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a756ac",
   "metadata": {},
   "source": [
    "Build and run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RF classifier that assess splits on mutual information\n",
    "rfc_mod = RandomForestClassifier(criterion = 'log_loss', n_jobs = 8, random_state = 888) \n",
    "# build the model on the training data\n",
    "rfc_mod.fit(X_train, y_train)\n",
    "\n",
    "# annotate the test dataset with predicted labels\n",
    "y_pred = rfc_mod.predict(X_test)\n",
    "\n",
    "# assess the accuracy of the model \n",
    "mod_acc = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('The model accuracy is {:.2f}'.format(mod_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a282643",
   "metadata": {},
   "source": [
    "Visualize the ROC and calculate the AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beaade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.RocCurveDisplay.from_estimator(rfc_mod, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (bootcamp)",
   "language": "python",
   "name": "python_bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
